Hi everyone, welcome back. This is the preservation week, and it's preservation integration parallels. So, I'm going to make a few more comments about preservation goals, linking our earlier discussion to goal classifications that we see elsewhere. And then, the bulk of this video will be looking at the relationship between preservation challenges and integration challenges. It's actually pretty interesting. Preservation challenges appear to be, in many ways, isomorphic to integration challenges. And tracing these parallels will help us better understand not only the challenges of preservation, but also the required interventions. So, a lot of what we're doing today will draw on our earlier discussion of data integration. So, first a few more words about preservation goals. And if you are familiar with literature at all, you know that many organizations and standards have proposed classifications of preservation goals, especially over the last 20 years. Bit of an industry. All right. First, an early and very simple classification of preservation goals, is this from around 2000, 2001, 2002. Preservation should make data viable, renderable, understandable. Viable, meaning we can read the ones and zeroes, the relevant, ones and zeroes, the relevant digital units from media. Renderable meaning we can process those bits into meaningful tokens. And understandable meaning that we can interpret those tokens appropriately. And here's a longer classification and It's one that is now very common and you'll see it. Viable, renderable, understandable, authenticatable, identifiable. So again, viable means that we can read the digital units from media correctly. Renderable means that those digital units can be viewed, processed, executed again correctly. Understandable meaning that we correctly understand the meanings of the tokens that we rendered. Authenticatable means that we can determine that the data we have is in fact what it purports to be, very important. And finally identifiable meaning that we can identify and re-identify the same data in different formats and from different sources. We know when we're using the same data that somebody else used. We know when we have the data that somebody else used. You can add more to this if you'd like. For instance, preservation should ensure that we can find relevant data. It should help us understand whether or not the data is conformant to some policies, rules and regulations and so on. And I want to emphasize a clause from our own definition. Each of these five goals must not only be achievable, but the user must have justified confidence in the result. For instance, it's not enough that you can read correctly the logical units from media, but you need to know that you are correctly reading the logical units from media. Reliability requires that. Now with that in mind, those goals in mind, I'm going to turn to what I call the data integration preservation isomorphism. Turns out that in many respects, the challenges of data preservation are the same as the challenges of data integration, only they are challenges across time rather than space. Interpreting space broadly here to mean social logical space. So, we're going to take a look at data preservation through that lens and see that we'll be able to apply many of the things we've already discussed. As always, and I'll repeat this frequently, metadata is a key instrument for achieving our objectives. That is, achieving our preservation goals. All right, we start with physical media. Natural place to start and ask the question, what is the role of physical media in communication with the future? So, even though as we've suggested earlier, preservation is not just about preserving physical objects. Nevertheless, physical objects are essential for communication with the future. And that's simply because we live in a world of space, time, energy, matter, causality. We live in a material world. And there's no communication without engaging with material things. Consequently, attending to physical objects is absolutely necessary to ensure communication with the future, even though that's not all there is to it. But the physical objects involved in communication with the future, need not persist through the entire temporal interval of some scenario of communication with the future. Communication with the future is achieved rather by, at least typically by a chain of overlapping physical objects. And I'm using physical objects very broadly here to include the patterns of energy as well. So, the functionality of physical objects does need to be maintained during the relevant intervals in this change. For instance, storage media and their associated hardware such as probes and cables must be protected against kinetic damage, breaking them, having them run over by the wheels on the bottom of your chair or burned up in a fire. They must be protected against chemical decay, electromagnetic impulse and so on, as well as of course loss and theft. For physical media the ensuring of communication with respect to these threats is accomplished by policies and procedures, the management of associated physical environments and, as always, accurate, complete, computer readable documentation. And I'll also mention although these are digital things, that the associated digital objects such as APIs, embedded systems, operating systems and so on also require attention as well. Moving on from physical objects to encoding. How do we communicate our encodings to the future? How do we ensure that our encodings will be correctly processed? So, at every level of encoding, the decoding of that level must be reliable. It must be possible to do it correctly and it must be reliable and known to be reliable. This includes, for instance, reading ones and zeroes off media, mapping at bit stream to bytes, mapping the bytes to integers, mapping integers to characters or other meaningful symbolic units. And you could make this more complicated if you like. Fundamental, if you're going to preserve information, if you're going to communicate with the future. And I know that we often these days ignore these problems because we have a lot of shared standards that simplify our lives, UTF, Unicode and so on. Things are easier than they were but nothing in this area can be taken for granted especially when we're looking 10, 20, 30 years in the future. And I'm sure all of you have discovered that you're not alwyas able to decode an encoding, often things that are very important to you. As always, we ensure the communication of encoding like we ensure all other communication through documentation, particularly, computer processable metadata that uses standard vocabularies and the standard serialization syntax. And usually, the most important role of this metadata is to indicate the various data standards being used at different levels of encoding or to indicate how we've extended or restricted or subseted in some other way modified those standards. Moving on from communicating coding to communicating syntax. And here what's especially important is a schema that identifies the structure of the data statements and that documents the controlled vocabulary of attributes, identifiers and values. As always, the schema should be computer processable so it can be used to validate the structure of the data, to supply data typesand also to configure processing tools and applications. And again, as always, we communicate communication. In this case, communication of syntax through documentation preferably computer processable metadata that uses standard vocabularies and a standard serialization syntax. And again often the most important role of this metadata is to indicate the various data standards that are being used by reference, typically not by inclusion. Now, moving on from syntax to propositions. And this of course is really what it's all about. That's why we're in the preservation business. That's why we're in the data science business. Ensuring reliable communication of what is being asserted, of what is being asserted in our terminology propositions. So, all of the prior attention that we've given to media encoding and syntax, that helps. Of course it's part of communicating propositions. But there's more to do at the level of communicating propositions. So, the constituents of propositions are relationships and entities or maybe some other similar things. And these are indicated in the controlled vocabulary of the top level logical syntax. But not just the vocabulary, but the constituent relationships and entities attributes and so on. These must be identified, explained and documented. And for that a formal model, conceptual model or an ontology is what's useful. The schema for your ontology should be, again, computer processable. So, it can be used for, in this case, validation, data integration and, very importantly, the management of transformation to alternative syntaxes which will be always on your list of things to do in data science. And without a high level conceptual schema, a propositional level schema, it's very difficult to manage those transformations with any degree of confidence in the result. Again, we're ensuring communication through documentation, computer processable metadata, standard vocabulary, standard serialization syntax. And again, the most important role of that metadata is often to indicate the various data standards that you're using. Here these would be standard ontologies or perhaps modified and the metadata will be indicating the nature of the modifications. At this level, however, natural language prose is probably also essential. Concepts like temperature, location, country, race, species, really, these things cannot be explained by formal language alone. Even the logic-based ontology languages that give us some traction on concepts like this, but then rarely complete the business of explaining precisely what we mean by temperature, location, country, race and so on. I'm not saying that we can never be exact and precise, but at the end of this, or maybe I should say at the foundation of this, natural language will be essential. And now I know what you're thinking, turtles again. And as in turtles all the way down, not turtles as in the turtle RDF serialization language. So, just as we saw with data integration that there is a kind of regress of documentation, the same regress appears in data preservation. Remember I said, we're looking at an isomorphism here, only across time rather than space. So, specifically metadata for data is itself data and so it must be preserved. And that will require, among other things, metadata for a metadata and so on. So, where we stop, and we usually stop pretty quickly, it's a practical matter. And when we stop, we will most likely be stopping with natural language prose. And you may be tempted to leave out that final description because you feel like everybody knows what you're talking about. Not a good idea. At some point, inevitably, a sound foundation for data preservation of your data sets will involve somebody in your organization not only creating metadata, but writing sentences in a natural language. This problem with the foundation is also why existing shared standards are so important. Shared standards already have a lot of natural language prose that backs up the mathematical formalities. And that's it for this video. Thank you. See you soon.