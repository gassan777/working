Okay, everyone this is the last week of data curation. This week we'll do a bit of a review. I'm not going to cover everything, I'm not going to cover every week and of course we're not going to cover everything but just hit some of what I think of as the highlights. And this first video is going to be short, addresses the question, data curation what is it? And you may remember, we break data science up into two components: data curation and data analytics. Data curation is ensuring that data can be efficiently and reliably found and used. And we really mean a lot by those two words. Efficiently means it's worth doing in a reasonable amount of time or for hit at the universe and all of that. And reliably means yeah, you can trust it. Data analytics, as you know, employs specific techniques to extract knowledge from data. But without preliminary and ongoing and post hoc data curatorial effort, data analytics is not going to get you anywhere. But I would say that one and why? Because it won't be sufficiently efficient to be worth using and it won't be trustworthy, your results won't be trustworthy. Okay and next as I like to point out, and this is fairly well documented, data curation is the larger part of data science in many respects. So, not only is data curation essential for a reliable efficient analysis, but most of the cost associated with using data is, by far, related to the curatorial support, that is data management and not the actual analysis. What's expensive is taking care of data and getting it ready for analysis and integrating it and preserving it and making sure that it's legal and clean it up, all that stuff. So this is also where most of our workforce needs are. And believe me you can ask any data manager in industry and they'll tell you that's where they're making the largest investment of money, staff time and effort. Let's get specific. You may remember this list, you've seen it a couple of times. These are areas of curatorial analysis, curatorial actions support collection, especially documenting the circumstances of collection, how that machine was calibrated, what model it is, what the parameters were set to, where exactly you were when it happened, all that stuff. Organization, making sure that the data is organized in an appropriate data model and that wherever possible, you're using relevant standards. Storage, that reliable, effective storage large problems won't ensue. Preservation, making sure that the data will be understandable and usable in the future by you or your company or your community. Discoverability, making sure that the data can be found. That is making sure that you can find relevant data, making sure that your colleagues can find the data that you produced. Can't find relevant data, it's not going to be any good to you and if your users cannot find the data you're producing, wont be any good for them. Access, we need to support the ability to retrieve and distribute data once we find it. You got to be able to get it. Workflow, you need to be able to develop transformation systems that you understand and that are efficient and that are reliable. Identification, you got to be able to know what data set you're looking at, so you can make sure it's the right one, that you know where it came from, that you can authenticate it, validate it and tell people what data you use to get a particular result. Integration, real world problems require data from multiple sources. Typically data that was not designed to be used together and probably not designed to be used by the user using it. All right, reformatting a lot of reformatting in data curation we're always taking data sets and updating them to new versions of the data format, reducing them in size, adding things to them, converting them from one data model to another. Without these things, we wont be able support our users and we won't be able to support new tools going forward. Reproducibility, particularly in science and engineering we need to know that the results we got we can get again from the same data with the same methods with the same software. A lot harder to assure ourselves of than one might think. Sharing, data is expensive. Everybody will be better off if we shared more of the important data that we developed. It's hard to support that, hard to encourage that and it's hard to support that. Communication, nobody knows about the data, what's the point? And actually succeeding in efficiently communicating or efficiently using data by studying it, not easy. It's a data deluge. You might have heard. Provenance, you need to know what inputs and what calculations are responsible for the data values that you're looking at. And sometimes you have to walk back through quite a number of transformations, identifying the methods, identifying the software, identifying the data inputs each step of the way, if you're going to understand and trust what you see. Modification, you got to correct and update that data. Compliance, don't want to go to jail, don't want to get sued. And of course, oh, my God, security, means of, yes, making sure your data is secure from tampering, making sure that it's secure from inappropriate access and distribution. Another problem much in the news and as you know, very costly. How do we do these things? Your five principle methods of data curatorial action: analysis, documentation, system design and implementation, policy, process. And throughout, especially in documentation, we're using computer accessible metadata. All right, that's first video in the review. Stay tuned for the next one.