Okay, data curation, this is review week. This is the third video, metadata preservation identity standards. Some highlights from the weeks on metadata, preservation, identity and standards. Metadata, you know, of course, the common definition, data about data. One thing to notice about this definition is that if it is a good one, and it's not bad, it makes it very clear that metadata is data. Again, metadata is itself data, and you've got to treat it like data. And all the things that we do and care about and try to accomplish when we take care of data, you got to do those things for metadata too. I know what you're thinking. That means we generate metadata about metadata. Where does it stop?. Well, you'll figure it out. We also, as you recall, wondered whether metadata was really a unique kind of data or whether it's just more data on the topic or the subject, just more information being collected. And decided that in many cases, it's hard to distinguish metadata from the data itself, but in some cases, metadata does seem to be distinctive. If you, for instance, if you're indicating the precision of a measurement, that's probably metadata in a unique sense. Standard classification of metadatas: descriptive, administrative, and structural. Descriptive includes, for instance, helping users find and understand and evaluate access data. Administrative is includes among, many other things, information that helps you decode and render data. So you're going to have a lot of information about format and data description and serialization and the semantics of it all. Metadata, it helps you with long-term management preservation and metadata about intellectual property rights and other regulatory constraints. And then there's structural metadata that we use to connect resources one with another. Make it clear when something is a part of something or associated with something. Really important to make this distinction. There are metadata schemas that are about the fundamental concepts, the attributes in question. And then there is the serialization of metadata instances in some serialization language or other. So, take a common example, the Dublin Core metadata set. The schema for this metadata set, it's really about the concepts, the attributes, and it doesn't indicate how you express them, how you actually attach an attribute value pair to an object. But there are different ways to do that. You can do it in RDF/XML. You can do it in HTML. You can do it with relational databases and so on. But it's an important distinction because you, for one reason or another, will frequently want to reformat one serialization into another serialization, preserving the same conceptual metadata. Sometimes, of course, you want to go from one conceptual metadata schema to a different conceptual metadata schema. You may recall we pondered for a bit an interesting issue involved with metadata, with attribute value pairs. Metadata typically attribute value pairs. We often see a record with a lot of attribute value pairs, and it looks like these are all about the same thing, but then we look closely and realize, they can't really be about the same thing. So, in this particular case, we see the attribute value pair author Matthew West, in both records, and there's something that was authored. And then we also see in one record pages, 408, and another pages, 389. So this thing authored by Matthew West, does it have 408 pages and 389 pages? Of course not. So what's going on here is that the attribute value pair pages and its value, that's about something different than what the author Matthew West attribute value pairs is about. In FRBR, terms author Matthew West. It's about a work and pages 389 pages for a right. Those are about different manifestations of the work. But that is not at all explicit formally in these records, which means that if you're going to do computation or manipulation, if you're going to do inferencing first, for instance, you're going to end up with errors. This works just fine when humans are looking at it, but it's not going to do for any kind of sophisticated data management. Metadata is everywhere in data curation. We went through this long list and the way we did this, metadata is part of practically every curatorial activity. Why? Because, basically, documentation and it's computer accessible, computer manipulatable documentation. Speaking of preservation, so you remember our data ontology? You might ask yourself, "When we're preserving data, what exactly are we preserving? Are we preserving crop-up digital content? Are we preserving simple structures? Are are we preserving pattern, matter, and energy?" You may recall from the week, kind of hard to say, and I ended up claiming, you're not preserving any of those things because preservation is not about preservation. Preservation not about preserving anything. It's not about preserving the existence of objects. I know it would be if you were making sure that a statue didn't rust or something, but that's not what's going on with digital data. What's going digital data is that we are supporting communication with the future. We're not preserving anything, unless it's the capability for business communication with the future, but that's preservation in an extended sense. So our simple definition of preservation is ensuring reliable communication with the future. And drilling down a bit on that, because I think it's got interesting, preservation actions are intended to ensure that future resources, one, will come into possession of physical media and encodings; two, from which they will correctly recognize the originally intended propositional content; three, and from which they will be justified in believing that this propositional content is, in fact, the intended propositional content. Yeah, you heard it from me first. A more traditional characterization of what's going on, the different kinds of things going on with preservation. You want to make sure that the data is viable. You couldn't actually get it off the tape. That it's renderable, you can render it in software and see it or execute it or process it. It's understandable. You know what you're looking at, what the meanings of those things are. Authenticatable, that means you can correctly determine that the data is what it purports to be. And then identifiable, you can identify it, and you can re-identify it. So how do you achieve preservation? These are the four common strategies. You'll see them mentioned everywhere. Replication, make lots of copies. Drew them about migration. Keep updating your data to new formats. New formats come out, update your data. Emulation, maintain software that behaves like a software your data is supposed to run in originally. Software's supposed to read and act on your data originally. And normalization, that's where we always convert data sets into a standard format that's optimized for preservation. This will typically be a format that uses standards for serializing data structures and, if possible, standard attributes and values, as well as standard schemas, but at least a standard syntax, documented of course. Okay. Transformations, transformations are important throughout data curation. Right here, we're looking at transformations that are involved in preservation. And both migration and normalization preservation strategies involve transformations. They involve transforming a data set in one format into a data set in another format. But they are migrating from format to format, keeping up with the Joneses, or whether you are migrating into and out of a standard interlingua format through the normalized of a special repository format. Either way, you're carrying out transformations. These transformations ideally should be documented. They should be documented in a standard language and, of course, a standard computer processable language. And the next slides are going to show you how that could happen. By the way, it's also an example of workflow and provenance documentation. And there it is. You see this example is one where we're using an XML-based language to document a particular transformation that was occurring. I don't actually know whether it was migration. I'm trying to figure out quickly. But maybe I can, whether it's migration or whether it's more like, sometimes it's hard to tell, whether you're looking at a migration or you're looking at a transformation out of canonical form. But you can puzzle out a bit what's going on here. But we're moving from an input file to an output file. Are you seeing XSL? You can't see it here but we see a reference to a XSLT template. And if you have this, you have excellent computer-readable documentation of when you did something, what you did, and how you did it. How cool is that? Okay, moving on to identity and identifiers. Lots of identification problems in data curation. You heard me go through them earlier. They crop up over and over. I'm not going to read this, same as you saw. And there are lots of things to be identified, everything from automobiles to proteins. So when it comes to datasets, what do we have to identify? And now that we have this nice little conceptual model for datasets, we can see exactly what kinds of things we might want to identify. You might want to identify a set of propositions, you might want to identify the top-level symbol structure that expresses those propositions, you might want to identify a second-level symbol structure that encodes the top-level symbol structure, and so on. A lot of stuff to identify and reidentify. And do you see it explicitly? That's your top-level proposition, you have your syntax level symbols, and you have symbols that indefinite number of encoding levels. And then finally, you reach the last abstract symbolic level just before you hit the real physical world. So it's hard, actually, to uniquely identify these abstract things, especially challenging to identify a set of propositions, but also challenging to identify the higher level symbol structures. Identifying a sequence of ones and zeroes, not so hard. After all, it's a sequence of ones and zeros. It's either a particular sequence or it's not. But with propositions and representations, we have this problem that there's one-to-many relationship between any particular level and the level below, and that can be a bit of a headache. Because we look at the lower level instantiations they can vary quite a bit. Remember it's one-to-many, even when in fact, they carry the same upper-level symbol structure, or whether or not they carry propositions. Because most of the time, what we really want to know is whether or not, say two scientists are using the same propositions, not whether they're using episodic versus ASCII. But there's a nice solution to this problem and that's canonicalization, which you know because you did a project. Canonicalization is a technique for determining a representational identity and as a consequence, it's a reasonable proxy for propositional identity. So by representational identity, I mean it's a way to determine whether or not two, let's say files, containing a particular data description in fact, contain the same description even when there are lots of incidental variations. And if they do contain the same data description, once you back out the incidental variations, the ones that don't really amount to a substitute difference, we can treat those two files as carrying the same, to use Rutgers terminology, scientific meaning, the same propositions, the same information. So the basic idea is the first thing you do is that the two files are in the same representation language. Then we're going to be going after propositional identity, we need to get them into the same representation language, so convert one to the language of the other or convert them both to a common language. Then you normalize these files by removing the incidental variations in the data description language. And then when you're done, you test the resulting files for byte-level identity. And after you've got byte-level identity after normalization, you assume identity of representation and identity of propositional content. Pretty cool. Okay, standards. I'm just going to say a little, tiny, short sermon on standards. Yeah, use them. Maybe I should stop there. Standards are fundamental in data curation, extremely important, extremely important. They do promote reliable, efficient communication with others now, that is on your team and across the street, and with ourselves, and others in the future. We support integration, interoperability, validation, authentication, preservation, regulatory compliance, and so on. And so far it's really, wherever you can you should be using standards. You're crazy if you don't. It will save you a lot of anxiety and money. And I just very quickly say, remember, that if you talk about conformance to standards, often, two kinds, data set conformance and processor conformance. Remember that because it's when you're worried about whether standards you're using, and sometimes the tendency to focus on data set conformance and not think about how the processors are going to change as time goes by, and what kind of interoperability over time you can expect. Okay, that's the third video in our whirlwind review of data curation. See you in a bit.