Welcome back, everyone. This is the fourth video of the review week, it's the last video of the review week, it's the last video of the course. And how appropriate that it's on Workflow and Provenance. Really important and pretty fascinating stuff. So, as always, remember what is questions, what is data workflow in this case? Most of our work with data and data curation and especially in scientific applications, but honestly, throughout the datasets workplace. Most of our work consists of transforming one dataset into another, day in day out, makes it sound like we're in a coal mine, doesn't it? Data curation is concerned with these transformations and is concerned with them in two ways. First, managing and documenting transformations because, we need to track what's going on with our data analytics colleagues and make sure that we can identify what they've done, how they did it, what the inputs were, what the outputs were. So that means that this is the management documentation of the transformations involved in data analytics. And then second, data curators have a lot of transformations to do themselves to realize their own data creation objectives. Objectives like preservation, integration, format conversion, and so on. Remember the three kinds of data transformations? The one where the input output datasets are identical. For instance in reformatting, switching from one data model to another, etc. Ones where the input dataset mathematically contains the output dataset, so these might be ordinary database queries or that might be statistical or other mathematical analysis. And then finally, cases where the transformation input dataset scientifically contains the output dataset. That's where you add some additional rules or facts about the world, that are then used in the transformation. This is how for instance, we get from information about one thing to information about another. My favorite example, how we get from information about air pressure to information about altitudes. It's not just mathematics, it's mathematics plus physics. And I'd like to show these diagram, here are some examples of workflows. Another example. Another example. So, when workflow is thought out carefully and when it's supported by software tools whether they're ones you make or ones you buy. You are moving towards certain objectives like efficiency, reliability, modifiability, reuse, reproducibility, these are all things that workflow systems, careful, well thought out, documented workforce systems help you achieve. Provenance, this is closely related to workflow. So the heart of computational provenance, these two questions: what data was used? What calculations were performed? and also, "what in the world exactly happened just now?" As the researcher stares at the output, puzzled, trying to figure out, how it is that the software tools, the functions, algorithms, inputs, produced what he is looking at? Why is provenance important? When we have access to information about provenance. Information about the inputs, the methods, tools, algorithms, we have a better understanding of our data, our data will be more reliable because we know how we got there. We know what to trust and what not to trust. We know how it came about, reproducibility is supported. Really important in science, trust. We also can manage attribution and credit, as you should do all this processing sometimes it can be hard to keep track of who you need to acknowledge, either explicitly and formally or actually sometimes who you need to pay. And managing provenance can also help you figure out what tools and algorithms are actually doing a good job. And which ones you want to use or what exactly they're good for, and whether you want to use them again in certain circumstances. So, the insights into how your tools are working and the ability to discover what data tools and algorithms are valuable also comes from managing provenance. You recall following Bertram literature, we distinguish two kinds of provenance in the workplace. Prospective, that's the documented workflow scenario. And then Retrospective, that's the data that's generated when you execute the workflow scenario. More pictures, this one reminding us that even scripts are workflow or they can document workflow. And in fact in real life, as you probably know, good documented scripts, sometimes that's the best we can do. Whatever you do, don't sit there at the command line just typing, typing in your reg axis or other transformations because those commands are now in the ether, and you can't go back a year later and figure out what you did, in what order, and why it produced what it produced. Few words about communication, why communication? Lots of reasons, but one is that, that's how data gets noticed and if your data is not being noticed, if it's not being used, what's the point? There is a kind of crisis, an interesting crisis I think, in scientific communication particularly in the journal, periodical, literature, at the problem, an incredible amount of information even around highly specialized topics. So, this has been a problem for a long time, but it's really reached the point now where you cannot specialize enough to restrict the amount of relevant data to a manageable size. So, the number of articles that you need to keep up with, unbelievable, they're going to the roof. The amount of time we spend reading articles, scientists spend reading articles, there's no more time left in the day. So scientists really can't spend a lot more time reading articles because there are only 24 hours, and there are a lot of other things to do so they're spending less and less time on articles. They're somehow managing to get through very sophisticated, challenging, complicated scientific articles really fast. How can that be? It's kind of scary. Okay, what can we do about this? How can we help the struggling physician, engineer, scientist, who has an enormous number of relevant, scientific articles that they have to engage with every week? One approach is, using text mining to extract information so they don't have to read those articles. They can't read them, there is not enough time. They've never really read them anyway. So instead of left to right, top-bottom reading, text mining software will provide information. This will and is delivering some assistance, but it's extremely unlikely that it's going to go all the way to solving the problem because the actual representation of what's going on with an experiment or the actual discursive analysis that a scientist provides just really cannot be easily boiled down to let's say, RDF propose that someone can browse. Life and science is more complicated than that. So we're always going to be reading, problem is, how can we manage our reading? How can we have tools that support our reading that get us through an article, as quickly as possible, getting what we want out of the article as quickly as possible? And so that of course brings us to tools for strategic reading. And there's some reason to believe that these tools, there's a lot of effort right now going into developing these tools, and there's reason to believe that this is the time when development of strategic reading tools which have been long, of course, hoped for, and thought about and experimented with. It looks like now we have the standards and also the ontologies, the domain ontologies in science that will be able to support tools, that will help us engage with scientific articles in a more productive way than we are now. Okay, let's wrap up this review, let's wrap up this course with my favorite quotations from Barend Mons, "We wouldn't have to mine the data if we didn't bury it in the first place." And data curation as much as possible it's our job to minimize the amount of desperate mining that our colleagues have to do because we've lost track of how we created our data, where data is, what it means, and so on.There's a lot of mining that has to take place, but let's not burden ourselves with mining that could have been avoided by using good curatorial practices. And finally, you may be seeing this for the third time, but can't do better. "Automate like you are going to live forever. Document like you're going to die tomorrow", Michael Sperberg-McQueen. Yeah, I'm pausing for a fact. Do you want me to read it again? I will. "Automate like you're going to live forever, in other words, write those scripts, it will save you time, it will save you confusion, it will save your frustration, but document like you're going to die tomorrow, and then your colleagues can figure out just exactly what it was that you did. And lastly, surf to this YouTube cartoon, it is hilarious. All right, that's it. I've had fun. I know this is a new course and this is the first iteration of it. And so, we're going to continue to evolve it, and I welcome all comments, and you can send them to me by e-mail or I don't know, give me a phone call or drop up my office if you're in town. If you'd like to talk about anything related to data creation, let me know. And again, I hope you enjoy the course and I hope you find it useful. And thanks for giving me this opportunity. Stay safe and have a good holiday. Bye for now.