Okay, welcome back. This is our world win review of some of the main ideas in the data curation course. This particular video is touching on some of the big ideas related to data models and data integration. Okay, and here's our slide of fundamental data model relationships. I do recommend that you make a laminated version and keep it in your wallet. I'm rather fond of how it represents the relationships between the conceptual logical physical levels and places at the appropriate level. The data models that we all know and love. But why talk about data models and of course, on data curation? Because, data models are really important to data curation. A lot of the work of people in data curation is around data models. We get to select them, you get to select kind of model you're going to use, you've got to find the schema or develop a schema. You will inevitably need to revise those models schemas. You must document those models schemas. And you must validate your instances against the schemas, your job. You need to transform data in one model into another, that is one kind of model into another, say from trees to relations, and you need to transform data in one schema to another schema even for the same data model. You also need to transfer data from one representation, one serialization, to another. That could be the same schema but the serializations differ. This is your life and an intellectually satisfying one. Finally, you need to integrate data from different data models, especially challenging, and I think, especially fun. All the way through, you need to document everything that you do and the documentation itself needs to be managed as data, so that you can perform computational actions on the documentation as well. Here are my nice little pictures of the data models we know and love. I'm sure you're familiar with these already. Lations and trees, the logical ones. Lation is great for attribute value pairs and tree is good for text documents. And conceptual one is good for tying it all together in a nice abstract human understandable way. Okay, here are the two fundamental problems that drove the development of data management systems. The first, programs and users were especially in the 50s and 60s, even in the 70s, interacting with data directly via its storage structure. And those storage structures varied widely and they were frequently changed, and this was a huge headache for everyone. Routines had to be rewritten, training didn't transfer, integration was nearly hopeless, extremely expensive, prohibitively expensive. More generally and somewhat abstractly, the nature of the information being stored was really not reflected in the management systems. We were just moving bytes around and characters around, and not really developing a robust conceptual, even logical picture of what we were doing. The systems did not reflect the attributes and relationships and so on that are actually the genuine components of the information being stored and managed. Here's some data storage representations, physical storage representations. You see how varied these representations can be that add variation which show up in the routines being written and the training that programmers got, and the documentation and the understanding. Do you get the point? It was a really unfortunate situation that we've all learned to avoid I think, but learn talking a little bit about how we learned to avoid it and what exactly is at the foundation of an improved approach to data serves us well. So, the problem is when you're interacting directly with data structures, directly with storage data structures, the results, the resulting systems, the resulting practices, inefficient, error-prone, untrustworthy, difficult to document, difficult to repurpose and reuse, difficult to preserve for future use. A lot depends upon the memory of staff and on workplace practices. And, a lot depends upon of course, custom tools, custom applications. I'm surprised we survived. So, a particular acute and intellectually interesting consequence of this chaos is failure of data independence comes in two varieties. First variety type one here, if the storage method changes, then the end user programs accessing the data will fail to perform as expected. Type 2, if new kinds of data need to be represented, then again, end user programs may fail or give the wrong results. Why? Because you're going right into that storage structure and accommodating new, you might say, accommodating new attributes, accommodating new columns, and these things are not anticipated by the existing tools. The results will be unpredictable. Okay, things were in a bad way but in 1970, Codd came along and said, look, the solution is simple. Forget about the storage structures, they're somebody's problem, they're not everybody's problem. Let's conceptualize data as relations, what mathematicians call relations tables, and then map those relations to whatever storage methods are being used. And then we deal with data as relations tables and somebody creates the mapping and somebody ensures the storage methods are working but it doesn't have to be the programmer and it certainly does it have to be the user. And this of course, changed the world. At the bottom of this approach, abstraction and indirection. The relational representation abstracts away from the specific and transient details of storage, presenting only the intrinsic features of the data itself. Interaction with stored data is thus indirect, it's via a mapping from the relational schemas to the stored data. And here's the picture, I'm sure you've seen a version of this picture before. What a relief, I got to tell you. All right. So, as we saw, the very same problems or problems for people working with documents and text in computer systems, there are many many ways to represent text and documents in computer systems. And in the 60s and 70s, interactions such as text processing with stored text was typically again, focused on storage structures or processing instructions. A slightly different wrinkle but in the end, has the same solution similar problem. What you didn't find of course, was an implementation of the fundamental principles of abstraction and indirection. They were not followed. Interaction was inappropriately focused on storage or processing instruction and all the usual problems ensued. Tools were not inoperable, versions of storage had to be constantly updated. Their integration was nearly impossible, it was a mess. So 1981, along comes Charles Goldfarb and proposes a simple solution. Interestingly similar to Codd's. Basically this, conceptualize a document as a tree of textual data elements and then map those elements to whatever storage or processing methods are needed. And this also changed the world. This was the birth of SGML, HTML, and later, XML. So, descriptive markup and trees, directly secret graphs with branches only one route. This is a model describing the logical components of documents in a well known data structure. It does not specify storage strategies. It does not specify processing, it abstracts away from storage and processing, and then, connects data to storage processing by mapping, such as by indirection. In other words, text and documents had the same problem and they had basically the same solution. And fundamentally, the solution is based on distraction, indirection. Here's a nice picture of a tree of textual data elements and then it's serialization in XML. So are we done? We get a nice logical data model for attribute value pairs. We have a nice logical data model for text and documents. We are not done. So, the old problem, the original problem with respect to post, both attribute value pairs and text. There are many different ways to use storage methods to store data. So we need a single data abstraction that allows us to work with data regardless of what storage methods are used. A relational model, and tree model do that. So, what's the problem? The new problem is that they're, listen to the parallels here, there are many different ways to use data abstractions to record information. So we need to information abstraction, that allows us to work with information, regardless of how the data expressing that information is stored. Not the same problem but a structurally isomorphic problem at a higher level, higher level abstraction. Who solved that? Peter Chen, in 1976. And I characterize his prescription this way. Conceptualize your domain of interest in terms of its things, its relationships etc., and then map that conceptualization to whatever logical model schemas are being used. Yes, or mapping to schemas, to logical model schemas. And this also changed the world, it was you might say, this is where we get your modeling specifically developed by Chen. It's where we get UML, and actually, it's the driving force behind the use of ontologies in data science and data management. And speaking of ontologies, there they are. You've got the ontology, about the dotted red-dashed red line and then blue, that dashed red line you've got an instance. Yes, I'm using RDF triple for the instance could use something else. Rectangles represent classes, the double arrows representing inside relationships, the named arrows represent relationships between things, dotted arrows are actual instances. And now we can review once more with improved understanding. Our pick diagram of data model relationships can get out that laminated card in your wallet. A little bit about data integration follows on i think quite naturally from our discussion of models, and data integration is a huge challenge today, and it's a major curatorial challenge. Data integration is not something your data analytics specialists are going to give you much help with, but unless it happens, you're not really going to get any significant traction on your business objectives, or the real world problems that you're trying to solve. Here's a quick definition. Data integration, combining data residing in different sources and providing users with a unified view. This is the problem, the so called unified view. Easy to say, hard to achieve. Why? Why heterogeneity? All kinds of heterogeneity. Some of these heterogeneities are today relatively easy to navigate or back there so he said navigate you don't even notice them. Others remain really challenging. So encoding heterogeneity, which basically different mappings from bitstreams into bytes, characters, numbers, etc. That these days relatively easy. Often we don't even notice when we have transformed, one bitstream to another different civilization, same characters. Who more two syntax heterogeneity, that's where we have different data description languages but different languages within the same type of model. So for instance, different RDF civilizations, Turtle, N3, XML/RDF, not really a big problem. But has to be done, and you never know when in fact it will turn out to be more problematic than it looks. Model heterogeneity. So this is when you need to combine data where the data is from different types of models. Relations from one source, RDF triples from another. Or an XML tree from one source, and RDF triples from another. Representational heterogeneity, that's when we get the same type of model but different choices being made by the modeler. So when modeler represents something with relationships, and another modeler represents the same phenomenon but with entities. Semantic heterogeneity is when you get similar features, perhaps very similar features but they're handled slightly differently. This can be a headache. Sometimes you don't even recognize you have a problem. Representational heterogeneity and semantic heterogeneity frequently involve schema integration. Multiple schemas, you're working at the schema level here, you are not working with the instance data. Working with the schema, trying to either develop a unifying schema, method schema, or converting to a new schema. Two neglected heterogeneities. Processing heterogeneity where maintenance and updates are handled differently and policy heterogeneity aimly differences regarding privacy security, intellectual property, licensing, ownership, and so on. This is challenging and it's tough. But it can also be a lot of fun. Okay. Next up, data concepts. So this is where we take a kind of I won't call it a side trip, but we take a really close look at very foundational issues in data curation, and actually in data in general. And I know it was a little abstract but it's good for you. It will pay off. Trust me. So our route into data concepts is to note that there are identity problems in many areas of data curation. We ask questions like, is this dataset already the archive? Was this information preserved in the new file format? Has this dataset been tampered with? Is this the data we think it is? Does this XML file have the same information as that JSON file? Were these datasets derived from the same data? Does the converted file have the same data as the original? And so on. And scientists are frequently pulled out their hair over problems like this. This here is a comment by Ruth Duerr of Snow and Ice. There are an unknown number of transformations that are invariant in the sense that they preserve the scientific meaning, preserve the information, you might say. But different scientific communities use different tools that require different representations. We have to generate those different representations with transformations preserving meaning. How do we know we've preserved the meaning? What does it even mean to preserve the meaning? So in a successful transformation, a successful conversion, something changes, right? It's a transformation, after all some big changes, but something remains the same. If nothing remains the same. You're in big trouble. So what is it that changes, and what remains the same? These are the kinds of somewhat abstract problems that are actually behind many challenges of data curation. So as you know, some researchers here at the University of Illinois, School of Information Sciences are working, and I was one of them, and our students developed a model intended to help us navigate in these issues. And here it is, called the Basic Representation Model, it's derived from FRBR. It has three entity types, propositional content, symbol structure, patterned matter and energy, that's the physical world right down there. And the basic idea is that propositions, meaning information if you will, is expressed in symbol structures. Those symbol structures are sometimes encoded by other symbol structures, which are sometimes encoded by other symbol structures unlike turtles. This goes on until eventually we have a symbol structure that's inscribed in the matter and energy of the physical world. An example, is propositions are expressed by RDF triples, RDF triples are encoded by RDF/XML, RDF/XML is encoded by unicode characters, unicode characters is encoded by UTF-8 bit streams, but UTF-8 bit streams because candidate as any to be inscribed in an actual RAID array state. So we posed the question, what is it in virtue of which a symbol structure expresses propositional content? What is that in virtue of which a symbol structure encodes a symbol structure? And in virtue of what does some matter and energy inscribe a symbol structure? And we call these things interpretative frames, and there things like standards, the ASCII standard, other ISO standards, data description, language standards, also agreements, conventions, even practices within communities of users, there are users. So, where have we gotten to with respect to saying what data is? Again, look at these three entity type rectangles, prepositional content, symbol structure, patterned matter and energy. Okay, we do have an answer. And our answer to the question, what this data is? Data are propositions, that's the top rectangle, that's entity type propositional content. Data are propositions asserted as evidence, how do you assert something? Symbols, and eventually patterned matter and energy. We stress that data is not a kind of thing, it's really a role that things of certain kinds assume. And this is an important distinction. So just as persons or students when they're enrolled in school, propositions are data when they are asserted as evidence. Being asserted as evidence is a contingent social circumstance just like being enrolled in school as is contingent on social circumstance. And so, data is a role that propositions have in certain contingent social circumstances. Data is not a special kind of thing. Proposition's that's the kind of thing we're dealing with data, but propositions that are in a particular role being asserted as evidence. And this let's make another important point, that is that data is relative. So, whether a proposition is data, or whether it's the claim that data is induced to support, really depends upon what's intended. So proposition's can be data in one circumstance, and a claim presumably backed up by data in another. And you know what, that's how science works as a collective human enterprise. So for climate scientists, growth rings on tree rounds, can be evidence for theories about temperature changes. But for an evolutionary ecologist, it's those theories about temperature changes that can be used as evidence for theories about competitive advantages. So you want a slogan? One person's data is another person's theory. And with that, it's the end of this video. See you soon.