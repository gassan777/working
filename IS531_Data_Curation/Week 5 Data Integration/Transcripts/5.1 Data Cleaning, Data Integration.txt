This week we focus on data integration. Very important part of data curation. This is the first video, so this is data cleaning and data integration. We're also going to talk specifically about heterogeneity, because that's a big problem in integration, and then different strategies for integration, in particular federation strategies versus drive combinations. So data cleaning. If you're involved in data management these days especially in the data analytics, you're probably involved in data cleaning and even if you aren't, you've probably read about it. Data cleaning is a general term used casually to refer to preparing data for analysis. This is an enormous part of using data these days. It takes up a lot of money, a lot of time, a lot of staff. If you don't get it right, things don't work, or they do work, but they give the wrong results especially embarrassing. It's extremely important. So data cleaning is to some extent distinguished from data integration, even though it's many ways some sense part of data integration very similar. But data cleaning typically is applied when you're working with a single schema data from a single source. And of course in data integration you'll be working with data from multiple sources, but from one source at a time often. So there's always data cleaning involved in data integration. So data cleaning typically suggests dealing with duplicate records. You got to do something with those. Get them down to one record. Values that are missing out of bounds, or inconsistent, data typing errors, or data typing inconsistencies, data entry errors, attribute interpretation errors. At some point somebody involved with the data set is confused about what the attribute actually means. Different approaches to handling nulls, fields with no values, misapplication of standards. Often standards are misunderstood, and applied in strange and troublesome ways, and sometimes the data developers know what's going on, and they just either don't care, or can't find a better solution. Sometimes they don't know that they're making a mistake. Inadequate normalization, of course, missing schemas, or inadequate schemas, missing relationships. Sometimes they're key to key relationships that you know were there, but they're not represented in the schema or elsewhere. Other kinds of failures of constraints like referential integrity. And of course often the data fails to match the schema, which may seem somewhat surprising since you get the schema often at the same time you get the data you would have presumed that, some kind of validation had occurred on its way to you. But guess what, the data doesn't match the schema. And now you need to figure out whether to remedy that in the data or remedy it in the schema. So as I said, data cleaning, profoundly important. Without a data can't be used reliably, or sometimes at all. Major expense. Okay. When multiple schemas are involved, or when you have instances of data that is data sets that are clearly disparate. If there are schemas for them it's unlikely that's the same schema. In these cases, the data preparation task is data integration. So, data integration, here's a well known definition: combining data residing in different sources and providing users with a unified view. It's a nice definition. Unified view of data in different sources. I've been wondering what does that mean, a unified view? We'll see in a minute. Here's our diagram. I hope you'd have had an opportunity to make a laminated copy. Probably should send them to you. Data integration is going to be something that will take place sometimes within a single logical model such as relations. Maybe all of your data sets are relational, but different schemas are clearly disparate. Sometimes data integration involves data from one or more logical models. You might have to integrate data, some of which is in relation, some in trees, some in triples. Why is it important? Why it's important is important. It's simply this, simply but importantly. Real world problems are profoundly interdisciplinary. They're not like the problems in the back of the book. They require integrating diverse data from multiple sources. Toy problems, problems in computer science classes, in database classes, in data science classes, typically do not involve diverse data from multiple sources, because we're learning things that are complicated enough. But you want to do something out in the real world in medicine, in commerce, in engineering, in crisis informatics. You're going to be assembling an enormous amount of data from many sources and you'll have a data integration problem. And be concrete, we're at the point home here. Imagine an impending natural disaster, an effective response can require understanding how many people will be affected. Hospital locations and capacity, transportation routes, the weather, needless to say, geology, hydrology, and so on. So many disparate databases will need to be accessed, demographic, meteorological, geographical, all kinds of things. That's why it's important. Why is it hard? Well, you know, why it's hard, you can guess why it's hard. Datasets developed by different communities for specific purposes are going to almost always be disparate in some way or other, and integrating them with other datasets is not typically anticipated by the developers. So these datasets often use different data models, different schemas, different encodings, and often these are very difficult to relate to each other. Hard to move the data from one type of data model into another, from one schema into another. Combine encodings, unify encodings. Nevertheless, less common concepts can be found that connect data across datasets, to either standardize or refactor related data elements, integration is not going to be possible. So the obstacle to data integration in a word, heterogeneity. Let's look at the different kinds of heterogeneity. Here they are. Thanks to Bertram Lud√§scher and his colleagues and also Amit Sheth. This is extended just to Bertram literature extended sets and I made some changes myself. And they're kind of in order here, you might say. Moving up in abstraction and out into the world. First reading from the top, encoding heterogeneity. So you often, of course, have different mappings from bitstreams into bytes, characters, numbers, or other logical units. This is not such a huge problem as it used to be thanks to Unicode and UTF. It's still a problem. Syntax heterogeneity. This is a blow up in abstraction, different data description languages for the same model type. So you might have already have an XML or you might have, already have an N3. Now encoding heterogeneity and syntax heterogeneity are relatively easy to deal with and there are for both of them typically already existing tools that work quite well. Model heterogeneity, a little harder. Here we have different model types. You might have relations in one dataset entities/relationships in another. So the relational tables in one case and you could have already have triples representing entities from relationships in another. Representational heterogeneity. So this is when you have different modeling choices within a model type. So for instance, within the model type entity, within an entity relationship model type, you may model phenomena as relationships, or you may model the same phenomenon as a kind of entity. You may say that a person is enrolled, that's relationship, in a school, that's how you say that a person is a student or you could have an entity type that is student. Simple example. Sometimes difficult certainly requires human intervention, unlike encoding heterogeneity but not too hard. Semantic heterogeneity. This is when we have different conceptualizations of the same or similar domain features, that is fundamentally different ways of thinking about temperature, for instance. Processing heterogeneity. So this is where you have different maintenance or update regimes for the data. So now we're not talking about how the data was produced, but how it's maintained, and different datasets may be updated at different times. If those datasets are linked because they're about the same domain but they're being updated at different times, as you can imagine, there can be unfortunate results. And finally, policy heterogeneity. This includes such things as different privacy rules, different security rules, varying ownership or licensing. You know these things are hard enough when you just have one dataset, and maybe one policy, or one legal system. But when you're combining data from multiple sources, things can get really hairy really fast, particularly if your data comes from different countries. Now the privacy security ownership licensing rules are all being sort of woven together. And good luck with that. But it's hard and we're not going to get away from it any time soon. Working this out is an ongoing research project, especially for international commerce. Okay, enough about data integration, what it is, why it's hard, why it's important, and kinds of heterogeneity. A few words in our last slide about how to address these problems. So there are two general approaches to data integration; federation, derivation. In Federation, we attach metadata to the various datasets that we are integrating and this metadata helps us see that the datasets are related, that they're relevant. And it also facilitates finding those datasets, retrieval. So for instance, you may want, you need to know that two or more datasets are about the same place, are about the same time, about the same species, about the same games. That can be nicely done by metadata on the datasets particularly if you have a standard metadata vocabulary and everybody uses that. They may organize data differently, they may use different attributes, but with a standardized metadata vocabulary, everyone can say this data is relevant to these places, these buildings, these animals, so on. But of course we want to actually do something with this data, not just find it and use the datasets individually. So for queries in a federated system, and this is the heart of federation. Views of the data that combine data from multiple datasets, unified views, and queries against multiple databases. Those are really the same thing. We support these by mappings to a mediating meta-schema. And if you think about it, you can imagine what that is. You may know already, of course, if you've done it in the workplace. But if all of these datasets have schemas with different attributes, what we want to do to unify them is map those various schemas to a single schema and then reviews in queries against multiple databases are done via the meta-schema mapping to individual schemas. So this approach that data, the separate datasets are not combined, they don't even have to be in the same system, they could be on different continents, for instance. Federation contrasts with derivation. This is where we create a single dataset from multiple sources and that dataset is governed by a single schema. So this you're probably familiar with and probably know as data warehouse, the data warehouse approach, the ETL approach of course. So these are the two approaches to data integration, but if you think about it, you'll realize of course that these two approaches don't actually directly solve the problem or even directly address the problem of heterogeneity. Heterogeneity as a problem is still there unaddressed in each approach. We'll address it differently in each approach. But so far we haven't scratched the surface. And that's the end of the first video this week on data integration. See you soon.