Hi there. Welcome back, everyone. This is the data integration week. And, at the last video, we talked about data integration. What is it? And there's a definition right at the bottom of the screen. Our definition combining data residing in different sources, and providing users with a unified view. We also talked about why it's hard, and why it's important, and classified kinds of heterogeneity, and then indicated the two fundamental approaches to managing heterogeneity, federation, and combination, the warehousing approach. But noted that those two approaches don't really address directly the heterogeneity problem. There are different ways of proceeding, but they still have the heterogeneity problem. So now, we're going to talk about how we address heterogeneity. So here are the kinds of heterogeneity that we've identified, and we're going to look at each of them, and talk about how they're dealt with. First, managing and coding heterogeneity. So the problem is there can be variations in character encodings, or byte interpretation, bitstream management, magnetic tape, blocking, formatting, and these create big problems. Not quite as big as it used to be in the bad old days, about years ago, but these things still create problems, and you need to know how to recognize them. Here's some examples. How many bits in an octet are actually coding? Coding the character or logical unit? Do you read the octet left to right or right to left? The sense problem. [inaudible] Are there control bytes in the bit stream? Where do the line end and carriage return characters? You may know there are some options there. So, if sets of logical items that need to be aligned are not identical, there may be no 1:1 conversion of their encodings, and variety of challenging problematic resolutions have to be considered. Conflation, unification, omission, or using named references, the &name; technique that you see in HTML. Sometimes, the issues with the logical items are really subtle and unclear. So, a U with two dots over it. We see that. We think we know what it is, but there are actually a couple different options for what it is. Is it a diaeresis, is it a umlaut? Those are two different things. You can't tell by looking. And of course, as you probably know, these things can be coded differently as well. Unicode is fairly effective at resolving both problems, really. So, things like Unicode, common tools and conventions like UTF-8, -16, mean that encoding is much less of an issue than it used to be. Still Be ready to encounter it now and then. Syntax heterogeneity. Converting from one common syntax to another within the same general data model, is actually an extremely common data curation activity. Do it day in, day out. So for instance, you may want to go from SGML to XML, or SGML to JSON, or from RDF/XML to N3, or turtle. Those are just various ways of doing RDF triples. They're all different, but SGML-XML, certainly variations within the general data model. JSON's debatable whether it fits that model or not, and RDF/XML, N3, turtle, those are all varying serializations for RDF. Schemas also may need to be converted from one schema language to another. Using XML as an example, we may need to go from the DTD schema language to the XML/Schema schema language, or XML/relaxNG, or Schematron, or any of the other languages. Some of them are grammar-based, like DTDs, which are basically extended back to a narrower form, and some of the schema languages are logic-based schematron. Still, it's within the same general data model, the tree data model. So these syntax conversions also can be challenging but in fact, there are common tools that work fairly well especially for the kinds of transformations you're most likely to want to do. The problems, however, that arise are sometimes insoluble without loss of information. So XML/Schema has a lot of data typing options. SGML/DTDs have grammar constraints that are very difficult to implement and XML/DTDs, those things are lost. Of course, in the case of going from SGML to XML, you lose the grammar constraints, but you gain a simpler grammar that allows you to use more tools. Okay. Model Heterogeneity. Here, things get a little more challenging. The problem is that the same information can be expressed using completely different types of data models. Same information can be expressed in relations, or trees, or ontologies. You might debate this, and I might debate this, but most of the time, most of us, especially in the workplace, we do assume that the same information can be expressed in different kinds of data models. Now, conversion of data from one data model to another is, of course, often needed. It's needed for data integration in general. It's needed for preservation. It's needed for exchange, and it's often needed simply to use particular tools, or particular applications that require a particular format. Some conversions that are particularly important data curation include exporting XML files from a relational database, using a relational database to store XML documents, say, going the other direction. Or, converting relations to RDF graphs, or converting RDF graphs to relations. So, for these particular conversions, most of the time methods exist, but they can be challenging to implement, and they may not provide the same functionality, data typing, or constraints, and you will have to deal with that. Sometimes, by the way, you'll move data into a particular format simply to check data typing constraints. Another problem with model heterogeneity is that some kinds of models are more natural than others for a particular kind of data. So if you imagine storing HTML web pages as relations, it can be done. There's a lot of mathematical equivalence here with these data models. It could be done, but it's not natural. It's very difficult to think about and work with text when it is stored as relations. When I say stored as relations, I don't mean dropping in into the Oracle blob, binary large object field. I mean, exploding all of the hierarchically structured data elements into relations. When you do these conversions, of course, often, you will need to develop a new schema. There may be a schema because you've already developed it, but often you'll be developing it for a particular conversion. This new schema will be, in some sense, an equivalent data model in the schema sense. And so, it'll be a challenge because you've got one schema from, say relations, and now you're going to generate a schema for RDF, or for XML, and you don't have the same resources. These approaches are different. And so, developing your schema can be a challenge. Okay. Representational heterogeneity. This is often challenging as well. So, here the problem is that even within a particular data model type, different modeling decisions are made that are, in some sense, equivalent, but they're not actually equivalent. So, in the case of XML, for instance, a classic decision that you have to make all the time when you're developing a schema, a DTD for instance, is whether or not to model a particular feature as an element, or as an attribute on an element. And within XML, there are pros and cons to your choice here. If you model the feature as an element, you'll be able to use the syntactic validation of an XML parser to locate, and confirm that your element is in the right place, if it's required that it's there. And you won't be able to do that if there is the value on an attribute. On the other hand, if it's the value of an attribute, you have a variety of data typing constraints that you can apply and do data typing, checking. There, that's an example of what might lead a XML/Schema designer to make one decision rather than another. And this particular case, there are actually a lot of different trade-offs between element and attribute. In a sense though, there's the same information being carried in each case. So, this is a case where there has been a decision about modeling, but it's not really a decision that represents a different view of the domain, simply a different representational strategy. Okay, another example. This one is within the data model type ontology. So, the question here is entity or relationship? You could, if you have students in your domain. Say, student is a type of entity and then you use a rectangle to represent the set of all students or the class of students. Alternatively, you could have the class of persons, the relationship enrolled in and the class of schools, all schools and represent someone being a student simply by the enrollment relation rather than membership in the student class. So they satisfy this dyadic predication enrolled in satisfying the first argument. This is also a modeling decision within a particular type of data model and it's not the case that we are seeing different things in the domain, this is a decision about how to represent the things we see in the domain and the two approaches appear to carry the same information. Now, actually, this is a case where there's some debate, could be debate as to whether the two cases carry the same information. If you see classes as representing fundamental kinds and you believe that student is not a fundamental kind, then you will think that there's something wrong, fundamentally wrong, with representing student as an entity type. So this is debatable, because whether or not ontology should be interpreted like that is debatable and I'll return to it, because say it's debatable, it's actually an interesting, very interesting discussion. But at least for most of us, most of the time, when we have two different ER models, two different ontologies, and they differ this way, it's the same information, but different representational strategies and we need to do some kind of schema alignment. Semantic Heterogeneity. This is absolutely challenging, so challenging, that I'm going to take it up to the next video. Okay, Processing Heterogeneity. This is a very interesting heterogeneity. It's important, but it's not well understood or studied much. The first examples are differences in modification protocols, which modification is a kind of processing. So differences in modifying the dataset. So this is instance level. Imagine, when dataset is updated weekly, another monthly, another one, observations change. Think about it. This can be a big problem for data integration. This can be a disaster for data integration. Why? So, presumably, of course, these datasets are about some of the same things. If they don't have some things in common. If they don't overlap in representations of a domain, then you can't combine them, you can't integrate them. They're independent. You can put them together, but they're not working together. So they've got to be about, for instance, some of the same people, or some of the same places, or some of the same music, or games, or whatever. If the integration is going to be anything more than para-tactic, but if there are on different updates schedules, then one of them is a picture of the world on Monday, another is a picture of the world on Friday, and you bringing them together. So inconsistencies, obviously, are likely and conclusions that you draw from the data are likely, are certainly possibly false. So update schedules can be a disaster, literally a disaster. There's a related problem with atomicity of transactions, if you remember that concept from your database course. You can think about that, on your own. So another problem is when datasets have different schedules for integrity checks and validation checks. This you might think why did this ever happen. But in fact it often does happen, it's often not practical to do constant real-time validation or integrity checks. And so you could have databases or datasets that aren't even syntactically correct and you're trying to combine them. Another example is when one dataset documents the provenance of modifications and another does not. So in this situation, you have a dataset that's doing things right, it's documenting provenance. We know why the data is what it is, but the data that we're combining and now is going to be integrated with that data. We don't know what transformations from what upstream inputs created that data. It's a mystery. So these are all these modification problems are especially, if you can tell from my examples, problems for Federated datasets. They ask the Federated approach to data integration, but they are also problems for derived or combined datasets, especially when the loading of the warehouses routine and frequent and automatic. Modeling differences, another kind of processing heterogeneity. Datasets can undergo schema changes without notification or coordination. And that can be a problem again. Obviously it's a huge problem in Federated datasets and it's really equally the same problem in combined derive combined datasets, because your routines, your scripts for combination are possibly, not possibly, almost certainly going to fail. And finally, Metadata differences. As metadata is updated, you can have a hard time finding a dataset you've been using and confirming that it is in fact the dataset that you have been using. And of course this interacts with these other problems, because you might want to know when it was last updated and you may or may not be able to. Of course, if you're human you read that metadata. But what we want is to manage these things computationally. So when different different metadata is being used to say the same things, you're going to have to federate metadata or use a combination, derivation technique as well as integrate the data itself. So, you've got two levels of data integration here and each level has, of course, two approaches: Federation, Combination. Policy Heterogeneity. So here, the problem is that information can be subject to different restrictions related to ownership, privacy, libel, security, disclosure, and so on. So, here's an example of a dataset, it could be federated or derived, could have originated in one country, be about the citizens of another, be owned by a firm and a third, stored in a fourth, accessed by users and others, and as a result the legal and regulatory requirements are going to be an enormous headache to puzzle out and to ensure compliance. They are also possibly even inconsistent. Compliance may be impossible. For the same reason civil obligations, contracts, licensing, and so on can be complex. And finally, data that you derive from other data, from multiple sources and here of course there may be a reduction. So a lot of data goes in, a little data goes out. What are the governing policies, laws, etc. on the data that comes out? This is a big headache.