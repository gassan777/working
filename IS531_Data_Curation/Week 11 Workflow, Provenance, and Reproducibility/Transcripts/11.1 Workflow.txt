Okay. Hi everyone, welcome back. This is the workflow and provenance week. Just three videos this time, one on workflow, one on provenance and a one on workflow systems. Provenance is kind of part of workflow. So, workflow systems includes provenance systems. You're not missing anything. I'm not missing anything. This is the first video on workflow. As always, I like begin with what is question, what is workflow? What is data workflow in particular? Why is workflow important? What kinds of transformations are there? Workflow is about transformations as we are about to see. So, what is workflow? Most of our work with data, especially scientific applications, but actually most of our work was data period consists, seems a little over simplifying but consists in transforming one dataset into another dataset. All day long. So in the abstract, software process embodying one or more algorithms takes one dataset as input, plus the algorithms and produces another dataset as output. So, this is the process that we call, here any way, workflow. These transformation scenarios, very widely in kind in nature and they can be extremely complex as you know. They're also fundamental to data science, absolutely at the heart of data science, and might say they're actually at the heart of computational science. They are a core focus of data creation, and that's for actually two reasons. So, data curation and data workflow, what are those two reasons? Data curation is concerned with transformations of data sets in two ways. First, data curation is concerned with managing and documenting the transformations that are involved in data analytics. And second, data curation is concerned with performing transformations in order to realize its own data curation objectives. And you know what that means because we've been over this several times now in preservation and integration and format conversion et cetera, et cetera. We are constantly transforming one dataset into another. Take a little closer look at data transformations abstractly, a little closer but also some still abstractly. I divide data transformations up into three kinds. So, the first kind are the transformations where the input output datasets are identical in propositional content. So, this is where for instance, the transformation is taking a dataset in one data description language and producing a dataset in another data description language but with the same information, the same propositional content. Simply moving from one description language to another. For instance be going from an RDF language to say a relational language. This first kind also includes transformations to a different serialization, that's where the input and output datasets not only carry the same propositional content, the same information, but they're also using the same description language but they're serializing it differently. Both of these are very prominent in anywhere, any data intensive workplace and make up a big part of anyone's job typically. The second kind is where the input dataset mathematically or perhaps logically contains the output dataset. And this includes those very simple transformations where you create an output dataset simply by querying the input dataset, with say elementary SQL relational queries. It also contains cases where the transformation creates say summaries, or statistics, or visualizations of the data in the input dataset. And we're familiar with both of these, of course, all of us I'm sure. In both cases whether it's transformation to a simple subset via queries or whether it's there are somewhat more calculation involved, in both cases, you might say that the input dataset mathematically contains the output dataset, or the input dataset mathematically entails the output dataset. The algorithms are making no assumptions about the world, they're simply doing logic and math and fairly simple math of that. A third kind of data transformation is where the input dataset scientifically contains the output dataset. And yes, I'm making up that phrase, scientifically contains the output dataset. What's that supposed to mean? So, this is a case where there is a transformation of the input dataset to an output dataset which says new things, substantially new things about the world and the information it contains, substantially different in kind from the input dataset, and this is accomplished by algorithms that make assumptions about what's going on in the world. For example, a dataset about air pressure can be transformed into a dataset about altitudes, instead of air pressure. Air pressure readings a certain longitude, latitude can be translated to a dataset about altitudes, and longitude latitude. Now a different kind of classification. We going to chop up workflow into the chaotic, the organized, and the supported. So, there's always workflow. There's always workflow. Whenever you're in the workplace, engaged with data, doing transformations, there's always workflow, even if it is an impenetrable, untrustworthy, chaotic mess, that's the workflow. Workflow can also be organized and organized workflows might be divided into two kinds. There's the home grown (systems of documented scripts) and homegrown systems can be a very high performing, very organized, very systematic, very reliable. There are also workflows that are supported by specialized workflow systems, such as Kepler, Taverna, YesWorkflow, et cetera. And practical workflow support systems these days are usually more or less language independent, so don't think you're going to write these off because you're fond of whatever you're fond of, or Python, whatever. Modern workflow systems are more or less language independent and that they allow you to use your favorite, your favorite languages, whether it's R,Python Python, XSLT, MATLAB, whatever. Now we can take a look at some graphic representations of workflows, and you can stare at these on your own time. I'm going to flip through them pretty fast and I think I owe all of them to Bertram Ludascher, who is a world class workflow provenance expert here at the School of Information Sciences. Here we have a nice simple one, well, simple in some respects and this involves genetics in particular taxonomy and alignment and take a look. You surf at the articles if you want to know more. Here is another example, this one, bioinformatics, still roughly in the taxonomy world here, here is one from DataOne. Wouldn't be complete without Microarray Data Analysis. So, here it is, GO, gene ontology, favorite of mine. And I'm going to end by repeating what I suspect is probably pretty obvious but worth reiterating. Why is workflow important? Because thoughtfully designed organized workflow support efficiency, reliability, modifiability, reuse, reproducibility, probably a couple other things. Efficiency, reliability, of course, two standard fundamental goals of data curation, probably most everything else can be deduced from those two, but to make it emphatic clear, these well-organized workflows are also easy to modify and you know what the modifications do. They're easy to reuse with different kinds of data, and you stand a much better chance of having results that can be reproduced by your colleagues. Very important if you're a scientist.