This video's titled, the problem. And what we are going to do is describe the historical situation that led to the development, or at least according to the story that we like to tell, led to the development of the relational model. Now, this is not just about the relational model. The historical situation, the problematic historical situation, that led to the relational model, is actually what is the, in many respects, the driving force behind the development and evolution of data models in general. So, it's not just about the relational model, but it's typically thought of as connected with the emergence of the relational model. So this is sort of a table of contents or a summary of this video. We are going to talk about the situation in the 1960s when data processing and computing had become extremely important. But we had not developed the culture of the protocols, the policies, the education, approaches, strategies that we have today, that we developed in order to address the problems that emerged in the '60s. Problems that had to do with the fact that data is stored in radically different ways, that interaction with data was typically immediately indirectly via storage methods, and that which vary, right? And that the explicit formal conceptualization of data as data was fairly rare. And to the extent that it happened was only in human memory or ad hoc documentation. So, why these six problems? We'll talk about that. In short, they created huge operational efficiencies. Talk about, why, how there was a tremendous, many lost opportunities, with respect to functionality, and there was the failure of what we call data independence. We'll talk about that. Okay. So, what's the problem? So, we imagine this is the 1960s, data processing is extremely important. In particular, imagine a large company with many divisions, and each division being pretty much left to itself, with respect to how it operates, and in particular, how it collects, stores and uses data. In this situation left to itself, each division is probably going to conceptualize their domain, whatever it is, on mobile parts, customers, finance. They're going to think about their domain in different ways. They're going to develop different methods of representing and storing data. And, inevitably, they're going to just mix up processing instructions for things like reports and data whenever it's convenient. They're just trying to get the job done. In this situation, there will typically be no clear separation between storage methods and the intrinsic structure of the information being represented, or even between processing instructions and data. And there will also be no abstract understanding of the nature of the information being represented. There's no culture of understanding and documenting, and thinking about what data is being represented, what kinds of relationships it has to work data internally, and also to the domain of representation, to the extent that there was a general abstract understanding during this period, it was likely to be in the heads, in the minds, in the memories of staff and programmers, are perhaps ad hoc documentation. Very lucky there was documentation. Without attention to the challenges of understanding data from a general abstract point of view. The complexity that a large organization has when its divisions are not coordinated is really enormous. So the problem at the heart of all of this is that, programs, computer programs, and users, are interacting directly with storage, and this storage is organized in a wide variety of ways. So, here are some of the ways, you could represent information in a computer system. Here's an example of the upper left, you see information organized in variable length fields, which you might say, "Okay, that's the same approach, right?" Well, sort of, but they use different delimiters, right? And tabs in one case, and commas in the other. On the right, you see fields being created through indexes such as byte offsets, and these are variable length and they're determined by indexes. At the bottom, you see fields determined by counting from the left or the right. These will be fixed-length fields, different lengths but fixed length for each. And finding out, where a field starts and where a field ends, will be a matter of counting. So, what's the problem with storing information as, to be stored somehow, in a wide variety of ways and allowing programmers and users to interact directly with it based on their knowledge of how it's stored? First of all, and focusing on software development, it was necessary to develop many different, unique, access subroutines in order to access this data. So, you had to develop them, test them, and maintain them. Let me remind you what we're dealing with here. Each of these storage representations one, two, three, four, could be used as the principal storage representation of data by a division. And that means, that programmers are going to be developing access routines for each, and yes, a division actually could have multiple storage representations. All four of these could be used by the same division. That means that you have to learn, for each database, exactly how the storage was being accomplished, and write a subroutine to access that database according to its storage strategy. So, the tools developed for differed divisions are not going to be interoperable, they're not going to be through a third party industry of common applications and tools, just too much variation for anybody to make any money. Specialized applications like searching analysis would have to be custom developed, very expensive and they will be able to reference constructs like fields, attributes and so on in a systematic way. And, perhaps most daunting is that whenever storage formats change often even a little, all the tools have to be modified. This created, despite all the value we were receiving from data processing in the 1960s, it really created enormous challenges as well, enormous inefficiencies. So, here are some workflow and transformation, very difficult to track, audit, or log. Documentation. You cannot exploit a general, conceptual understanding of data, concepts like records or fields or attributes not available when we're looking at this. Sure, you can develop them for each but without a systematic understanding of how you're going to characterize the higher level of data understanding, you're not going to be successful. Documentation has to be updated for frequent changes in storage format, processing changes, data validation. Quality assurance is going to be exceptionally difficult because there aren't going to be standard tools for syntax. Checking syntax, what's that? Data typing since constraint management, none of those things. None of those things that we depend upon for ensuring that our data is reliable for managing risk and safety. Schemas to the extent that they exist, all focus just on storage and will lot help us with general data management. So, the result. The systems and practices, and probably we're exaggerating the situation in the 1960s, but it's idealized story. So, the systems and practices in data processing in 1960s as result of the focus on storage of the allowing users and application programs or partnering users and the application programs to interact directly with storage. Systems and practices were inefficient, error prone, untrustworthy, difficult to document, difficult to repurpose-reuse, difficult to preserve for future use, dependent on memory and workplace practices, and depended on custom tools and applications. Man I don't know how we survived. One of the interesting significant consequences of this chaos is what in Computer Science we call a failure of data independence and this failure comes in two varieties. In type one, if the storage method changes, then the end user programs that access the data fail to perform as expected and in type two, if new kinds of data need to be represented, then again, end user programs may fail or give the wrong result. And, this lack of data independence was in some ways the, you know what gave a lot of insight into what was wrong in the problematic situation and what direction the remedy lay. So, I want to say one more about data independence and to start, keep these variations in storage strategies in mind. Okay, now a closer look at a type one, failure of data independence. This is a failure where if the physical storage method changes, then the end user programs accessing the data will fail to perform as expected and here's how that can happen. So, for instance, if the storage method switches from a fixed field approach to a delimited field approach, then obviously, the access programs whatever they are, are going to be returning the wrong results. They're not going to be finding the values in the fields that they're supposed to be finding. Whether those programs are designed to update the database or they're being used by the CEO to track what's going on, they're not going to work. You change the storage method and all of the programs even ones that are designed for the receptionist to check a phone number, they're going to fail. Lack of data independence type two. Here, if new kinds of data need to be represented, then again, end users may fail to give the wrong results. That is, we're in a situation where we're not able to change our system in order to accommodate new kinds of data without risking failure for existing access programs. And here's how that can happen. So, for instance, if a new attribute is accommodated by adding a delimited field to the right side of a record. Then, any program or other tool that has been identifying fields by counting delimiters right to left is going to return the wrong results. There is no systematic, general understanding of how access programs are supposed to work in order to accommodate the addition of new kinds of information. And you know, conversely, there's no understanding of how we add new kinds of information in ways that will ensure that the existing programs will continue to work as expected. As long as these programs are interacting directly with stored data, with the storage methods being used as part of the access strategy, you're going to have these kinds of problems. The next video, we will show how the relational model emerged from this problematic situation and took on these problems directly and solve them pretty much.